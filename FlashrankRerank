from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.query_constructor.base import AttributeInfo
from langchain.chains.router import MultiRetrievalQAChain
from langchain.chains.llm import LLMChain
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.retrievers.flash_retriever import FlashRetriever
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from typing import List, Any, Dict, Optional

# Set up the OpenAI API key (you need to replace 'YOUR_API_KEY' with your actual API key)
import os
os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"

# Load your documents
loader = TextLoader("path_to_your_document.txt")
documents = loader.load()

# Split the documents into chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Create embeddings for the documents
embeddings_model = OpenAIEmbeddings()
db = FAISS.from_documents(texts, embeddings_model)

# Define the retriever
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# Define the FlashRankRerank retriever
flash_retriever = FlashRetriever(
    base_comparator=retriever,
    query_constructor_chain=LLMChain(
        llm=ChatOpenAI(), 
        prompt=PromptTemplate(
            input_variables=["field_info", "input_text"],
            template="{input_text}"
        )
    ),
    # Optionally provide other parameters like `llm` or `query_constructor_chain`
)

# Create a QA chain using the FlashRankRerank retriever
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",
    retriever=flash_retriever,
    return_source_documents=True
)

# Query the chain
query = "What is the main topic of the document?"
result = qa_chain({"query": query})
print(result['result'])
print(result['source_documents'])
